{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWNtiFv5ejh9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeoPw36P_8TT"
      },
      "outputs": [],
      "source": [
        "! pip install rasterio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgekW7vdQL3e"
      },
      "outputs": [],
      "source": [
        "!apt install gdal-bin -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhoPC2omVUMI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import requests\n",
        "import zipfile\n",
        "from osgeo import gdal, osr, ogr\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import subprocess\n",
        "from rasterio.warp import reproject, Resampling\n",
        "from rasterio.windows import Window\n",
        "from concurrent.futures import ThreadPoolExecutor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh2Yz1UaVWZC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Directory to extract files\n",
        "extract_dir = \"/content/drive/MyDrive/GHS_POP_DATA\"\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Function to download, unzip, and delete zip file\n",
        "def download_and_extract(url, extract_to):\n",
        "    local_filename = url.split(\"/\")[-1]\n",
        "    # Download the file\n",
        "    response = requests.get(url)\n",
        "    with open(local_filename, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(local_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    # Delete the zip file\n",
        "    os.remove(local_filename)\n",
        "\n",
        "# Process each URL\n",
        "for year in range(1975,2030,5):\n",
        "    url = f\"https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_POP_GLOBE_R2023A/GHS_POP_E{year}_GLOBE_R2023A_4326_30ss/V1-0/GHS_POP_E{year}_GLOBE_R2023A_4326_30ss_V1_0.zip\"\n",
        "    download_and_extract(url, extract_dir)\n",
        "\n",
        "# List the extracted files\n",
        "extracted_files = os.listdir(extract_dir)\n",
        "print(extracted_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgx3IMhxsORS"
      },
      "outputs": [],
      "source": [
        "#build vrt\n",
        "folder_pop = \"/content/drive/MyDrive/GHS_POP_DATA\"\n",
        "# List all files and directories in the specified folder\n",
        "tif_files = glob.glob(os.path.join(folder_pop, '*.tif'))\n",
        "os.chdir(folder_pop)\n",
        "vrt_path ='GHS_POP.vrt'  # path to vrt to build\n",
        "ds = gdal.BuildVRT(vrt_path, tif_files, options=gdal.BuildVRTOptions(separate=True,srcNodata=-200, VRTNodata=np.nan))\n",
        "ds.FlushCache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja5lWjx4JNZ4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OygMd3Oulfq-"
      },
      "outputs": [],
      "source": [
        "######for stats start here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scS5x_ma-lJB"
      },
      "outputs": [],
      "source": [
        "df_wpp_prediction = pd.read_csv(\"/content/drive/MyDrive/CCRI/WPP2022_prediction.csv\")\n",
        "df_wpp_estimate = pd.read_csv(\"/content/drive/MyDrive/CCRI/WPP2022_estimate.csv\")\n",
        "df_wpp = pd.concat([df_wpp_estimate,df_wpp_prediction], ignore_index=True)\n",
        "country_bnd = gpd.read_file('/content/drive/MyDrive/CCRI/global_bnd_adm0.geojson')\n",
        "\n",
        "# Merge polygons with the same ISO3 code\n",
        "country_bnd = country_bnd.dissolve(by='iso3')\n",
        "country_bnd = country_bnd.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_4tTx3L_XB"
      },
      "outputs": [],
      "source": [
        "# Path to the VRT file\n",
        "pop_vrt_path = \"/content/drive/MyDrive/GHS_POP_DATA/GHS_POP.vrt\"\n",
        "file_1960s = \"/content/drive/MyDrive/hwi_stats/dgca/average_hwi_1960s.tif\"\n",
        "file_2020s = \"/content/drive/MyDrive/hwi_stats/dgca/average_hwi_2020s.tif\"\n",
        "#1km resolution\n",
        "# Create the output directory if it doesn't exist\n",
        "output_dir = \"/content/drive/MyDrive/POP_stat\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEvC_7Aysxlb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aKW1Ek1sxhl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from shapely.geometry import mapping\n",
        "from rasterio.mask import mask\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from shapely.geometry import MultiPolygon\n",
        "import tempfile\n",
        "import fiona\n",
        "\n",
        "\n",
        "def calculate_exposure(iso3, pop_vrt_path, file_1960s, file_2020s, df_wpp, year, output_dir):\n",
        "    \"\"\"Calculates exposure metrics for a given ISO3 code and writes results to a CSV file.\"\"\"\n",
        "    print(f\"Processing {iso3}\")\n",
        "    child_percent = df_wpp.loc[(df_wpp['ISO3 Alpha-code'] == iso3) & (df_wpp['Year'] == year), '0-17']\n",
        "    if child_percent.empty:\n",
        "      print(f\"No population data for {iso3} in {year}\")\n",
        "      return None\n",
        "\n",
        "\n",
        "\n",
        "    filtered_gdf = country_bnd[country_bnd['iso3'] == iso3]\n",
        "    # Create a temporary filtered GeoJSON file\n",
        "    filtered_geojson_path = f'/content/drive/MyDrive/POP_stat/filtered_{iso3}.geojson'\n",
        "    filtered_gdf.to_file(filtered_geojson_path, driver='GeoJSON')\n",
        "\n",
        "\n",
        "    subset_pop_path = os.path.join(output_dir, f\"{iso3}_pop_subset.tif\")\n",
        "    subset_T1_path = os.path.join(output_dir, f\"{iso3}_T1_subset.tif\")\n",
        "    subset_T2_path = os.path.join(output_dir, f\"{iso3}_T2_subset.tif\")\n",
        "\n",
        "    def clip_raster(input_path, output_path, filtered_geojson_path, iso3_value):\n",
        "\n",
        "      # Clip the raster using gdalwarp with the filtered GeoJSON file\n",
        "      warp_options = gdal.WarpOptions(\n",
        "          cutlineDSName=filtered_geojson_path,\n",
        "          cropToCutline=True,\n",
        "          dstAlpha=True,\n",
        "          format='GTiff',\n",
        "          creationOptions=[\"COMPRESS=LZW\"]\n",
        "      )\n",
        "\n",
        "      gdal.Warp(destNameOrDestDS=output_path, srcDSOrSrcDSTab=input_path, options=warp_options)\n",
        "\n",
        "    # Clip each input raster to the subset paths\n",
        "    try:\n",
        "        clip_raster(pop_vrt_path, subset_pop_path, filtered_geojson_path, iso3)\n",
        "        clip_raster(file_1960s, subset_T1_path, filtered_geojson_path, iso3)\n",
        "        clip_raster(file_2020s, subset_T2_path, filtered_geojson_path, iso3)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during raster clipping: {e}\")\n",
        "        return None\n",
        "\n",
        "    data_T1, data_T2 = {}, {}\n",
        "    pop_band = (year - 1975) // 5 + 1\n",
        "    try:\n",
        "        with rasterio.open(subset_pop_path) as pop_src:\n",
        "            total_pop = np.nansum(pop_src.read(pop_band))\n",
        "            child_percent = df_wpp.loc[(df_wpp['ISO3 Alpha-code'] == iso3) & (df_wpp['Year'] == year), '0-17']\n",
        "            if child_percent.empty:\n",
        "                print(f\"No population data for {iso3} in {year}\")\n",
        "                for path in [subset_pop_path, subset_T1_path, subset_T2_path]:\n",
        "                    os.remove(path)\n",
        "                return None\n",
        "            child_percent = float(child_percent.values[0])\n",
        "            child_pop = pop_src.read(pop_band) * (child_percent / 100)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {subset_pop_path}: {e}\")\n",
        "        for path in [subset_pop_path, subset_T1_path, subset_T2_path]:\n",
        "                    os.remove(path)\n",
        "        return None\n",
        "\n",
        "    def read_data(file_path, data_dict):\n",
        "        try:\n",
        "            with rasterio.open(file_path) as src:\n",
        "                data_dict['hw_count'] = src.read(1)\n",
        "                with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                    hw_count = src.read(1)\n",
        "                    hw_days = src.read(2)\n",
        "                    hw_days_per_count = np.where(hw_count == 0, 0, hw_days / hw_count)\n",
        "                    data_dict['hw_days'] = hw_days_per_count\n",
        "                data_dict['hw_temp_diff'] = src.read(3)\n",
        "                data_dict['high_temp_degree_days'] = src.read(4)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    read_data(subset_T1_path, data_T1)\n",
        "    read_data(subset_T2_path, data_T2)\n",
        "\n",
        "    results = {'iso3': iso3}\n",
        "    results[\"total_pop\"] = total_pop\n",
        "    results[\"child_pop\"] = np.nansum(child_pop)\n",
        "    results[\"child_percent\"] = child_percent\n",
        "\n",
        "    for key in data_T1.keys():\n",
        "        percentage_increase = np.zeros_like(data_T1[key], dtype=float)\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            data_T1_key = data_T1[key]\n",
        "            data_T2_key = data_T2[key]\n",
        "             # Case 1: Both T1 and T2 are zero\n",
        "            both_zero_mask = (data_T1_key == 0) & (data_T2_key == 0)\n",
        "\n",
        "            percentage_increase[both_zero_mask] = 0\n",
        "\n",
        "            # Case 2: T1 is zero and T2 is not zero\n",
        "            T1_zero_T2_nonzero_mask = (data_T1_key == 0) & (data_T2_key != 0)\n",
        "            percentage_increase[T1_zero_T2_nonzero_mask] = np.inf\n",
        "\n",
        "            # Case 3: T1 is not zero\n",
        "            T1_nonzero_mask = data_T1_key != 0\n",
        "            percentage_increase[T1_nonzero_mask] = ((data_T2_key[T1_nonzero_mask] - data_T1_key[T1_nonzero_mask]) / data_T1_key[T1_nonzero_mask]) * 100\n",
        "\n",
        "            for threshold in [50, 100, 200]:\n",
        "                mask_thresh = percentage_increase > threshold\n",
        "                results[f\"exposure_{key}_{threshold}\"] = np.nansum(child_pop[mask_thresh])\n",
        "\n",
        "    # Clean up temporary subset files (optional)\n",
        "    for path in [subset_pop_path, subset_T1_path, subset_T2_path, filtered_geojson_path]:\n",
        "         os.remove(path)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zouVzxO5emih"
      },
      "outputs": [],
      "source": [
        "# Parallel processing setup\n",
        "results_list = []\n",
        "iso3_codes = country_bnd['iso3'].unique()\n",
        "year = 2025\n",
        "max_workers = 3\n",
        "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "    futures = {executor.submit(calculate_exposure, iso3, pop_vrt_path, file_1960s, file_2020s, df_wpp, year, output_dir): iso3 for iso3 in iso3_codes}\n",
        "\n",
        "    for future in futures:\n",
        "        result = future.result()\n",
        "        if result is not None:\n",
        "            results_list.append(result)\n",
        "\n",
        "# Combine results and write to a single CSV\n",
        "all_results_df = pd.DataFrame(results_list)\n",
        "all_results_df.to_csv(os.path.join(output_dir, \"all_exposure_results_high_res_1975.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QL9llYQDFcmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}